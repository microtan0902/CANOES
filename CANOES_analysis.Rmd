---
title: "CANOES analysis in detail"
output: html_notebook

---

*Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 
*Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.
*Preview* button or press *Cmd+Shift+K* to preview the HTML file.

# DEPENDENCIES
#    nnls, Hmisc, mgcv, plyr
```{r}
# read in the data
gc <- read.table("gc.txt")$V2
canoes.reads <- read.table("canoes.reads.txt")
# rename the columns of canoes.reads
sample.names <- paste("S", seq(1:26), sep="")
names(canoes.reads) <- c("chromosome", "start", "end", sample.names)
# create a vector of consecutive target ids
target <- seq(1, nrow(canoes.reads))
# combine the data into one data frame
canoes.reads <- cbind(target, gc, canoes.reads)
# call CNVs in each sample
# create a vector to hold the results for each sample
xcnv.list <- vector('list', length(sample.names))
```

```{r}
canoes.reads
```

# Constants
```{r}
NUM.ABNORMAL.STATES=2
NUM.STATES=3
DELETION=1
NORMAL=2
DUPLICATION=3
```

# CallCNVs
#     Calls CNVs in sample of interest
以S1为例子
```{r}
sample.name=sample.names[1]
counts <- canoes.reads
p=1e-08
Tnum=6
D=70000
numrefs=30
get.dfs=F
homdel.mean=0.2

if (!sample.name %in% names(counts)){stop("No column for sample ", sample.names, " in counts matrix")}
if (length(setdiff(names(counts)[1:5], c("target", "chromosome", "start", "end", "gc"))) > 0){
    stop("First five columns of counts matrix must be target, chromosome, start, end, gc")
}
if (length(setdiff(unique(counts$chromosome), seq(1:22)))>0) {
  stop("chromosome must take value in range 1-22 (support for sex chromosomes to come)")
}
if (p <= 0){
  stop("parameter p must be positive")
}
if (Tnum <= 0){
  stop("parameter Tnum must be positive")
}
if (D <= 0){
  stop("parameter D must be positive")
}
if (numrefs <= 0){
  stop("parameter numrefs must be positive")
}
sample.names <- colnames(counts)[-seq(1,5)]
# find mean coverage of probes,这是每个sample RC的平均，再对每个sample平均的平均，就是一个数。全总均值
mean.counts <- mean(apply(counts[, sample.names], 2, mean))
mean.counts
```

```{r}
# 对每一个样本标准化，使他们总体RD基本相等，具有可比性
# normalize counts; round so we can use negative binomial
# apply.2 对每一列进行处理.apply的第四个参数为optional arguments to FUN.
  original_sum <- colSums(counts[, sample.names]) 
  counts[, sample.names] <- apply(counts[, sample.names], 2, 
        function(x, mean.counts) round(x * mean.counts / mean(x)), mean.counts)
  current_sum <- colSums(counts[, sample.names]) 
  current_sum
```

```{r}
# calculate covariance of read count across samples
  cov <- cor(counts[, sample.names], counts[, sample.names])
  reference.samples <- setdiff(sample.names, sample.name)
  covariances <- cov[sample.name, reference.samples]
  reference.samples <- names(sort(covariances, 
          decreasing=T)[1:min(numrefs, length(covariances))])
  sample.mean.counts <- mean(counts[, sample.name])
  sample.sumcounts <- apply(counts[, reference.samples], 2, sum)
```

```{r}
  # 进一步的标准化，使reference samples之间的总体RD相等
  # normalize reference samples to sample of interest
  counts[, reference.samples] <- apply(counts[, reference.samples], 2, 
        function(x, sample.mean.counts) 
                round(x * sample.mean.counts / 
                mean(x)), sample.mean.counts) 
  colSums(counts[, reference.samples])
```

We aim to weight reference samples more highly to the extent they share the systematic read count biases of sample j. To accomplish this, we regress the read count data of sample j against the read count data of the reference samples using non-negative least squares regression (using the nnls package in R), which constrains the coefficients to be positive and minimizes multicollinearity（多重共线性）. 
```{r}
# select reference samples and weightings using non-negative least squares
  b <- counts[, sample.name]
  A <- as.matrix(counts[, reference.samples])
  library(nnls)
  all <- nnls(A, b)$x #all为Nonnegative least squares model的所有系数
  est <- matrix(0, nrow=50, ncol=length(reference.samples)) #50行，reference samples列空矩阵
  set.seed(1)
  for (i in 1:50){
    d <- sample(nrow(A), min(500, nrow(A)))#在1到nrow中随机选取500个数，抽样来代表全部。重复50次，来代表总的权重sample.weights
    est[i, ] <- nnls(A[d, ], b[d])$x  
  }
  head(est)
```

Because we have normalized the read count data so that all reference samples have the same aggregate read count as sample j, the coefficients also sum to 1. We then use these coefficients as the weights wk.
```{r}
  weights <- colMeans(est)
  sample.weights <- weights / sum(weights)
  sample.weights
  sum(sample.weights)
```

```{r}
library(Hmisc)
# calculate weighted mean of read count
# this is used to calculate emission probabilities
counts$mean <- apply(counts[, reference.samples], 
                     1, wtd.mean, sample.weights) # 每个target的权重均值。wtd.mean refers to 'Weighted Statistical Estimates'
counts$mean
targets <- counts$target
  
# exclude probes with all zero counts
nonzero.rows <- counts$mean > 0
nonzero.rows.df <- data.frame(target=counts$target, 
                              nonzero.rows=nonzero.rows)
counts <- counts[nonzero.rows, ]

```
```{r}
# returns data frame with distance to each target from the previous target 
# (0 in the case of the first target on chromosome 1, a very big number
# for the first target on each other chromosome--this resets the HMM
# for each chromosome)
GetDistances <- function(counts){
  chromosome <- counts[, "chromosome"]
  startbase <- counts[, "start"]
  num.nonzero.exons <- length(startbase)
  distances <- c(0, startbase[2:num.nonzero.exons] - 
                   startbase[1:(num.nonzero.exons - 1)] + 
                   1000000000000 * (chromosome[2:num.nonzero.exons] - 
                                      chromosome[1:(num.nonzero.exons - 1)]))
  return(data.frame(target=counts[, "target"], distance=distances))
}

  # get the distances between consecutive probes
  distances <- GetDistances(counts)
  
```
When the effective number (taking into account the weighting discussed above) of reference samples used to estimate σ2ij is low, the prediction of σ2ij may be inaccurate and sometimes may be even lower than μij, implying variance of read count less than Poisson, in which case the variance would equal the mean. We therefore set a floor on the estimate of σ2ij equal to the higher of the mean read count at target i (μij) and s2ij. s2ij
 is a prediction of σ2ij based on the observed relationship between the sample variance and two covariates, the sample mean and the GC content. We use this floor to improve the prediction of s2ij by incorporating information from all the targets with similar mean sequence depth and GC content to target i (14).

The variance of read count of targets in the reference samples is observed to increase in a curvilinear fashion with the mean read count and to be higher at both high and low GC content (see Figure 2). To establish a floor on the estimate of σ^2_ij at a target i of mean read count μ and GC content g, we regress the variance for the targets against both (i) the GC content of the targets and (ii) the mean read count across the reference samples for the targets with a generalized additive model, using the mgcv package in R (18).
```{r}
EstimateVariance <- function(counts, reference.samples, sample.weights){
  library(Hmisc)
  counts$var <- apply(counts[, reference.samples], 1, wtd.var, sample.weights, normwt=T)#是对每一行（target）求加权方差
  set.seed(1)
  counts.subset <- counts[sample(nrow(counts), min(36000, nrow(counts))), ]#取最大为36k target的子矩阵
  library(mgcv)
  # can't do gamma regression with negative 
  counts.subset$var[counts.subset$var==0] <- 0.1 
  dim(counts.subset)
  # var、mean、gc为counts中的列，代表每一行target的对应的方差、均值、gc；
  # 下面的回归应该是代表的是每一个target的var由reference sample的加权mean和该target的gc作为协变量
  # Okay, 模型的均值mu和方差s实际上并不等于在data中观测到的均值mean和方差var；
  fit <- gam(var ~ s(mean) + s(gc), family=Gamma(link=log), data=counts.subset)
  fit1 <- gam(mean+0.001 ~ s(mean) + s(gc), family=Gamma(link=log), data=counts.subset)
  # we don't want variance less than Poisson
  # we take maximum of genome-wide estimate, method of moments estimate
  # and Poisson variance
  # 取每一target的方差，如果是泊松分布的方差（泊松分布认为均值=方差）
  v.estimate <- pmax(predict(fit, counts, type="response"), counts$var, 
                     counts$mean * 1.01)
  
  m.estimate <- predict(fit1, counts)
  return(data.frame(target=counts$target, var.estimate=v.estimate))
}

# estimate the read count variance at each probe
  var.estimate <- EstimateVariance(counts, reference.samples, 
                                               sample.weights)

```

```{r}
# for test a sample
test.counts <- counts[, sample.name]
target.means <- counts$mean
var.estimate <- var.estimate$var.estimate
targets <- counts[, "target"]

EmissionProbs <- function(test.counts, target.means, 
                                      var.estimate, targets){
  num.targets <- length(test.counts)
  # calculate the means for the deletion, normal and duplication states
  # 就是把每一个target的mean取1/2、3/2作为del和dup的mean
  state.target.means <- t(apply(data.frame(x=target.means), 1, function(x) c(x*1/2, x, x*3/2)))
  # calculate the expected size (given the predicted variance)
  size <- target.means ^ 2 / (var.estimate - target.means)
  emission.probs <- matrix(NA, num.targets, 4)
  colnames(emission.probs) <- c("target", "delprob", "normalprob", "dupprob")
  # calculate the emission probabilities given the read count
  size.del <- size
  size.dup <- size
  size.del <- size / 2
  size.dup <- size * 3 / 2
  emission.probs[, "delprob"] <- dnbinom(
    test.counts,
    mu=state.target.means[, 1],
    size=size.del, log=T)
  emission.probs[, "normalprob"] <- dnbinom(
    test.counts,
    mu=state.target.means[, 2],
    size=size, log=T)
  emission.probs[, "dupprob"] <- dnbinom(
    test.counts,
    mu=state.target.means[, 3],
    size=size.dup, log=T)
  emission.probs[, "target"] <- targets
  # some values may be infinite as a result of extreme read count
  row.all.inf <- which(apply(emission.probs, 1, function(x){all(is.infinite(x))}))
  if (length(row.all.inf) > 0){
    for (i in row.all.inf){
      if (test.counts[i] >= state.target.means[i, 3]){
        emission.probs[i, 2:4] <- c(-Inf, -Inf, -0.01)
      }
      else if (test.counts[i] <= state.target.means[i, 1]){
        emission.probs[i, 2:4] <- c(-0.01, -Inf, -Inf)
      }
      else emission.probs[i, 2:4] <- c(-Inf, -0.01, -Inf)
    }
  }
  return(emission.probs)
}

head(emission.probs)
```

### RT comments：
CANOES是假设RC在每一个target服从NB分布，对每一个target上的RC进行参数估计。参数s受GC和RC均值影响，作为协变量进行回归。
我认为其目的是可以更加精准的获取每一个target上的模型，从而对该区域的CN可以更好的进行预测。
- 缺点：理解难度大，且实际效果与以样本为单位，先GC校正之后再取非极端的RC进行参数估计的实际效果没有比较研究。

```{r}
  emission.probs <- EmissionProbs(counts[, sample.name], 
                        counts$mean, var.estimate$var.estimate, 
                        counts[, "target"])
  if (get.dfs){
    return(list(emission.probs=emission.probs, distances=distances))
  }
  # call CNVs with the Viterbi algorithm
  viterbi.state <- Viterbi(emission.probs, distances, p, Tnum, D)  
  # format the CNVs
  cnvs <- PrintCNVs(sample.name, viterbi.state, 
                         counts)
  # if there aren't too many CNVs, calculate the Q_SOME
  if (nrow(cnvs) > 0 & nrow(cnvs) <= 50){
    qualities <- GenotypeCNVs(cnvs, sample.name, counts, p, Tnum, D, numrefs, 
                          emission.probs=emission.probs, 
                          distances=distances)
    for (i in 1:nrow(cnvs)){
      cnvs$Q_SOME[i] <- ifelse(cnvs$CNV[i]=="DEL", qualities[i, "SQDel"], 
                               qualities[i, "SQDup"])
    }
  }
  data <- as.data.frame(cbind(counts$target, counts$mean, var.estimate$var.estimate, counts[, sample.name]))
  names(data) <- c("target", "countsmean", "varestimate", "sample")
  if (nrow(cnvs) > 0){
    cnvs <- CalcCopyNumber(data, cnvs, homdel.mean)
  }
  return(cnvs)
}
```





# Viterbi algorithm
Viterbi <- function(emission.probs.matrix, distances, p, Tnum, D){
  targets <- emission.probs.matrix[, 1]
  emission.probs.matrix <- as.matrix(emission.probs.matrix[, 2:4])
  num.exons <- dim(emission.probs.matrix)[1]
  viterbi.matrix <- matrix(NA, nrow=num.exons, ncol=NUM.STATES)
  viterbi.pointers <- matrix(NA, nrow=num.exons, ncol=NUM.STATES)
  initial.state <- log(c(0.0075 / NUM.ABNORMAL.STATES, 1 - 0.0075, 0.0075 / NUM.ABNORMAL.STATES))
  viterbi.matrix[1, ] <- initial.state + emission.probs.matrix[1,]
  for (i in 2:num.exons) {
    temp.matrix <- viterbi.matrix[i - 1, ] + GetTransitionMatrix(distances$distance[i], p, Tnum, D)
    viterbi.matrix[i, ] <- apply(temp.matrix, 2, max)
    emission.probs <- c(emission.probs.matrix[i,])
    dim(emission.probs) <- c(NUM.STATES, 1)
    viterbi.matrix[i, ] <- viterbi.matrix[i, ] + emission.probs
    viterbi.pointers[i, ] <- apply(temp.matrix, 2, which.max)
  }
  viterbi.states = vector(length = num.exons)
  viterbi.states[num.exons] = which.max(viterbi.matrix[num.exons, ])
  for (i in (num.exons - 1):1) {
    viterbi.states[i] <- viterbi.pointers[i + 1, viterbi.states[i + 1]]
  }
  return(data.frame(target=targets, viterbi.state=viterbi.states))
}

# returns a transition matrix
#                              to state
#                    deletion   normal    duplication
#           deletion   
#from state   normal
#        duplication
GetTransitionMatrix <- function(distance, p, Tnum, D){
  q <- 1 / Tnum
  f = exp(-distance/D)
  prob.abnormal.abnormal <- f * (1 - q) + (1 - f) * p
  prob.abnormal.normal <- f * q + (1 - f) * (1 - 2 * p)
  prob.abnormal.diff.abnormal <- (1 - f) * p
  prob.normal.normal <- 1 - 2 * p
  prob.normal.abnormal <- p
  transition.probs <- 
    c(prob.abnormal.abnormal, prob.abnormal.normal, prob.abnormal.diff.abnormal, 
      prob.normal.abnormal, prob.normal.normal, prob.normal.abnormal,
      prob.abnormal.diff.abnormal, prob.abnormal.normal, prob.abnormal.abnormal)
  transition.m = log(matrix(transition.probs, NUM.STATES, NUM.STATES, byrow=TRUE))
  return(transition.m)
}

# adds two log-space probabilities using the identity
# log (p1 + p2) = log p1 + log(1 + exp(log p2 - log p1))
AddTwoProbabilities <- function(x, y){
  if (is.infinite(x)) return (y)
  if (is.infinite(y)) return (x)
  sum.probs <- max(x, y) + log1p(exp(-abs(x - y)))
}

# adds multiple log-space probabilities
SumProbabilities <- function(x){
  sum.probs <- x[1]
  for (i in 2:length(x)){
    sum.probs <- AddTwoProbabilities(sum.probs, x[i])
  }
  return(sum.probs)
}

# finds the data likelihood by summing the product of the corresponding 
# forward and backward probabilities at any token (should give the same value
# regardless of the token)
GetLikelihood <- function(forward.matrix, backward.matrix, x){
  SumProbabilities(forward.matrix[x, ] + backward.matrix[x, ])
}

# get the forward probabilities
GetForwardMatrix <- function(emission.probs.matrix, distances, p, Tnum, D){
  emission.probs.matrix <- as.matrix(emission.probs.matrix[, 2:4])
  num.exons <- dim(emission.probs.matrix)[1]
  forward.matrix <- matrix(NA, nrow=num.exons, ncol=NUM.STATES)   # matrix to hold forward probabilities
  initial.state <- log(c(0.0075 / NUM.ABNORMAL.STATES, 1 - 0.0075, 0.0075 / NUM.ABNORMAL.STATES))
  forward.matrix[1, ] <- initial.state + emission.probs.matrix[1, ]
  for (i in 2:num.exons){
    # compute matrix with probability we were in state j and are now in state i
    # in temp.matrix[j, i] (ignoring emission of current token)
    temp.matrix <- forward.matrix[i - 1, ] + GetTransitionMatrix(distances$distance[i], p, Tnum, D)
    # find the probability that we are in each of the three states
    sum.probs <- apply(temp.matrix, 2, SumProbabilities)
    forward.matrix[i, ] <- sum.probs + emission.probs.matrix[i, ]
  }  
  return(forward.matrix)  
}

# get the backward probabilities
GetBackwardMatrix <- function(emission.probs.matrix, distances, 
                                  p, Tnum, D){
  emission.probs.matrix <- as.matrix(emission.probs.matrix[, 2:4])
  num.exons <- dim(emission.probs.matrix)[1]
  backward.matrix <- matrix(NA, nrow=num.exons, ncol=NUM.STATES)   # matrix to hold backward probabilities
  initial.state <- log(c(0.0075 / NUM.ABNORMAL.STATES, 1 - 0.0075, 0.0075 / NUM.ABNORMAL.STATES))
  backward.matrix[num.exons, ] <- rep(0, NUM.STATES)
  for (i in (num.exons - 1):1){
    temp.matrix <- GetTransitionMatrix(distances$distance[i+1], p, Tnum, D) + 
      matrix(backward.matrix[i + 1, ], 3, 3, byrow=T) +
      matrix(emission.probs.matrix[i+1, ], 3, 3, byrow=T)
    backward.matrix[i, ] <- apply(temp.matrix, 1, SumProbabilities)
  }  
  final.prob <- backward.matrix[1, ] + emission.probs.matrix[1, ] + initial.state
  return(backward.matrix)  
}

# find the likelihood of the data given that certain states are disallowed
# between start target and end target
GetModifiedLikelihood <- function(forward.matrix, backward.matrix, emission.probs.matrix, distances, 
                                      start.target, end.target, disallowed.states, p, Tnum, D){
  targets <- emission.probs.matrix[, 1]
  emission.probs.matrix <- as.matrix(emission.probs.matrix[, 2:4])
  # there may be missing targets in this sample, we genotype the largest stretch of 
  # targets that lie in the CNV
  left.target <- min(which(targets >= start.target))
  right.target <- max(which(targets <= end.target))
  num.exons <- dim(emission.probs.matrix)[1]
  unmodified.likelihood <- GetLikelihood(forward.matrix, 
                                             backward.matrix, min(right.target + 1, num.exons))
  #right.target or left.target may be empty
  
  #if (right.target >= left.target) return(c(NA, unmodified.likelihood))
  stopifnot(right.target >= left.target)
  modified.emission.probs.matrix <- emission.probs.matrix
  modified.emission.probs.matrix[left.target:right.target, 
                                 disallowed.states] <- -Inf
  
  # if the start target is the first target we need to recalculate the 
  # forward probabilities
  # for that target, using the modified emission probabilities
  if (left.target == 1){
    initial.state <- log(c(0.0075 / NUM.ABNORMAL.STATES, 1 - 0.0075, 0.0075 / NUM.ABNORMAL.STATES))
    forward.matrix[1, ] <- initial.state + modified.emission.probs.matrix[1, ]
    left.target <- left.target + 1
  } 
  for (i in seq(left.target, min(right.target + 1, num.exons))){
    # compute matrix with probability we were in state j and are now in state i
    # in temp.matrix[j, i] (ignoring emission of current token)
    temp.matrix <- forward.matrix[i - 1, ] + GetTransitionMatrix(distances$distance[i], p, Tnum, D)
    # find the probability that we are in each of the three states
    sum.probs <- apply(temp.matrix, 2, SumProbabilities) 
    if (!i == (right.target + 1)){
      forward.matrix[i, ] <- sum.probs + modified.emission.probs.matrix[i, ]
    } else{
      forward.matrix[i, ] <- sum.probs + emission.probs.matrix[i, ]
    }
  }  
  # find the modified likelihood of the sequence
  modified.likelihood <- GetLikelihood(forward.matrix, backward.matrix, min(right.target + 1, num.exons))
  return(c(modified.likelihood, unmodified.likelihood))
}

SummarizeCNVs <- function(cnv.targets, counts, sample.name, state){
  sample.name <- sample.name
  cnv.type <- ifelse(state==3, "DUP", "DEL")
  cnv.start <- min(cnv.targets$target)
  cnv.end <- max(cnv.targets$target)
  cnv.chromosome <- counts[cnv.start, "chromosome"]
  cnv.start.base <- counts[cnv.start, "start"]
  cnv.start.target <- counts[cnv.start, "target"]
  cnv.end.base <- counts[cnv.end, "end"]
  cnv.end.target <- counts[cnv.end, "target"]
  cnv.kbs <- (cnv.end.base - cnv.start.base) / 1000
  cnv.midbp <- round((cnv.end.base - cnv.start.base) / 2) + cnv.start.base
  cnv.targets <- paste(cnv.start.target, "..", cnv.end.target, sep="")
  cnv.interval <- paste(cnv.chromosome, ":", cnv.start.base, "-", cnv.end.base, sep="")
  num.targets <- cnv.end.target - cnv.start.target + 1
  return(data.frame(sample.name=sample.name, cnv.type=cnv.type, cnv.interval=cnv.interval, 
                    cnv.kbs=cnv.kbs, cnv.chromosome=cnv.chromosome, 
                    cnv.midbp=cnv.midbp, cnv.targets=cnv.targets, num.targets=num.targets))
}

PrintCNVs <- function(test.sample.name, viterbi.state, 
                      nonzero.counts){  
  consecutiveGroups <- function(sequence){
    num <- length(sequence)
    group <- 1
    groups <- rep(0, num)
    groups[1] <- group
    if (num > 1){
      for (i in 2:num){
        if (!sequence[i] == (sequence[i - 1] + 1)) group <- group + 1
        groups[i] <- group
      }
    }
    return(groups)
  }
  num.duplications <- 0
  num.deletions <- 0
  for (state in c(1, 3)){
    cnv.targets <- which(viterbi.state$viterbi.state == state)
    if (!length(cnv.targets) == 0){
      groups <- consecutiveGroups(cnv.targets)
      library(plyr)
      cnvs.temp.df <- ddply(data.frame(target=cnv.targets, group=groups), 
                            "group", SummarizeCNVs, nonzero.counts, test.sample.name, 
                            state)
      if (state == 1){
        deletions.df <- cnvs.temp.df
        if (!is.null(dim(deletions.df))){
          num.deletions <- dim(deletions.df)[1]
        }
      } else {
        duplications.df <- cnvs.temp.df
        if (!is.null(dim(duplications.df))){
          num.duplications <- dim(duplications.df)[1]
        }
      }
    }
  }
  num.calls <- num.deletions + num.duplications
  cat(num.calls, "CNVs called in sample", test.sample.name, "\n")
  if (num.deletions == 0 & num.duplications == 0){
    df <- data.frame(SAMPLE=character(0), CNV=character(0), INTERVAL=character(0), 
                     KB=numeric(0), CHR=character(0), 
                     MID_BP=numeric(), TARGETS=character(0), NUM_TARG=numeric(0), Q_SOME=numeric(0), MLCN=numeric(0))
    return(df)
  }
  if (num.deletions > 0 & num.duplications > 0){
    cnvs.df <- rbind(deletions.df, duplications.df)
  } else {
    ifelse(num.deletions > 0, 
           cnvs.df <- deletions.df, cnvs.df <- duplications.df)
  }
  xcnv <- cbind(cnvs.df[, c("sample.name", "cnv.type", "cnv.interval", 
                      "cnv.kbs", "cnv.chromosome", "cnv.midbp", 
                      "cnv.targets", "num.targets")], 0)
  colnames(xcnv) <- c("SAMPLE", "CNV", "INTERVAL", "KB", "CHR", "MID_BP", "TARGETS",
                      "NUM_TARG", "MLCN")
  xcnv$Q_SOME <- NA
  return(xcnv)
}

CalcCopyNumber <- function(data, cnvs, homdel.mean){
  for (i in 1:nrow(cnvs)){
    cnv <- cnvs[i, ]
    targets <- as.numeric(unlist(strsplit(as.character(cnv$TARGETS), "..", fixed=T)))
    cnv.data <- subset(data, target >= targets[1] & target <= targets[2])
    state.target.means <- t(apply(data.frame(x=cnv.data$countsmean), 1, 
                                  function(x) c(C1=x*1/2, C2=x, C3=x*3/2, 
                                                C4=x * 2, C5=x * 5/2, C6=x*6/2)))
    # calculate the expected size (given the predicted variance)
    size <- cnv.data$countsmean ^ 2 / (cnv.data$varestimate - cnv.data$countsmean)
    emission.probs <- matrix(NA, nrow(cnv.data), 7)
    colnames(emission.probs) <- c("C0", "C1", "C2", "C3", "C4", "C5", "C6")
    #colnames(emission.probs) <- c("target", "delprob", "normalprob", "dupprob")
    # calculate the emission probabilities given the read count
    emission.probs[, 1] <- dpois(cnv.data$sample, homdel.mean, log=T)
    for (s in 1:6){
      size.state <- size * s/2
      emission.probs[, s+1] <- dnbinom(cnv.data$sample, mu=state.target.means[, s], 
                                       size=size.state, log=T)
    }
    cs <- colSums(emission.probs)
    ml.state <- which.max(cs) - 1
    if (ml.state==2){
      ml.state <- ifelse(cnv$CNV=="DEL", 1, 3)
    }
    cnvs$MLCN[i] <- ml.state
  }  
  return(cnvs)
}
